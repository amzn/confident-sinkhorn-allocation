{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reproduce_experiments_Confident_Sinkhorn_Allocation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtPgo3/7rRPWVjA2/lTaHn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ntienvu/confident_sinkhorn_allocation/blob/master/reproduce_experiments_Confident_Sinkhorn_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tubZzNQc3EFg",
        "outputId": "bc50632b-8f67-4c3d-b8ee-932f158e7862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://ntienvu:****@github.com/ntienvu/confident_sinkhorn_allocation\n",
            "  Cloning https://ntienvu:****@github.com/ntienvu/confident_sinkhorn_allocation to /tmp/pip-req-build-75zn9xz4\n",
            "  Running command git clone -q 'https://ntienvu:****@github.com/ntienvu/confident_sinkhorn_allocation' /tmp/pip-req-build-75zn9xz4\n",
            "Collecting colorama>=0.4.5\n",
            "  Using cached colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting cycler>=0.11.0\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting fonttools>=4.33.3\n",
            "  Using cached fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
            "Collecting joblib>=1.1.0\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting kiwisolver>=1.4.3\n",
            "  Downloading kiwisolver-1.4.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 30.0 MB/s \n",
            "\u001b[?25hCollecting matplotlib>=3.1.2\n",
            "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 8.1 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.21.0\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 227 kB/s \n",
            "\u001b[?25hCollecting packaging>=21.3\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting pandas>=1.2.3\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 29.0 MB/s \n",
            "\u001b[?25hCollecting Pillow>=9.2.0\n",
            "  Using cached Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Collecting pyparsing>=3.0.9\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 43.2 MB/s \n",
            "\u001b[?25hCollecting pytz>=2022.1\n",
            "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 43.7 MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=1.0\n",
            "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8 MB 1.9 MB/s \n",
            "\u001b[?25hCollecting scipy>=1.7.1\n",
            "  Using cached scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "Collecting six>=1.16.0\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting tqdm>=4.64.0\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 151 kB/s \n",
            "\u001b[?25hCollecting xgboost>=1.6.1\n",
            "  Using cached xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
            "Collecting typing-extensions\n",
            "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: csa\n",
            "  Building wheel for csa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for csa: filename=csa-1.0-py3-none-any.whl size=38519 sha256=049b2678446f45e6157f76cba300591d09fab46e638b9945db155638ef3ed7b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o4flbdhl/wheels/f8/a6/d2/85775e4ad67413a2107706e416bca603ec61caa57a672bd339\n",
            "Successfully built csa\n",
            "Installing collected packages: typing-extensions, six, pyparsing, numpy, threadpoolctl, scipy, pytz, python-dateutil, Pillow, packaging, kiwisolver, joblib, fonttools, cycler, xgboost, tqdm, scikit-learn, pandas, matplotlib, colorama, csa\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2022.1\n",
            "    Uninstalling pytz-2022.1:\n",
            "      Successfully uninstalled pytz-2022.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.2.0\n",
            "    Uninstalling Pillow-9.2.0:\n",
            "      Successfully uninstalled Pillow-9.2.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.3\n",
            "    Uninstalling kiwisolver-1.4.3:\n",
            "      Successfully uninstalled kiwisolver-1.4.3\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.33.3\n",
            "    Uninstalling fonttools-4.33.3:\n",
            "      Successfully uninstalled fonttools-4.33.3\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 1.6.1\n",
            "    Uninstalling xgboost-1.6.1:\n",
            "      Successfully uninstalled xgboost-1.6.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: colorama\n",
            "    Found existing installation: colorama 0.4.5\n",
            "    Uninstalling colorama-0.4.5:\n",
            "      Successfully uninstalled colorama-0.4.5\n",
            "  Attempting uninstall: csa\n",
            "    Found existing installation: csa 1.0\n",
            "    Uninstalling csa-1.0:\n",
            "      Successfully uninstalled csa-1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.0.17 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n",
            "spacy 3.3.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-9.2.0 colorama-0.4.5 csa-1.0 cycler-0.11.0 fonttools-4.33.3 joblib-1.1.0 kiwisolver-1.4.3 matplotlib-3.5.2 numpy-1.21.6 packaging-21.3 pandas-1.3.5 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.1 scikit-learn-1.0.2 scipy-1.7.3 six-1.16.0 threadpoolctl-3.1.0 tqdm-4.64.0 typing-extensions-4.3.0 xgboost-1.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "colorama",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall git+https://ntienvu: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@github.com/ntienvu/confident_sinkhorn_allocation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from algorithm.pseudo_labeling import Pseudo_Labeling\n",
        "from algorithm.flexmatch import FlexMatch\n",
        "from algorithm.ups import UPS\n",
        "from algorithm.csa import CSA\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from utilities.utils import get_train_test_unlabeled,append_acc_early_termination\n",
        "from utilities.utils import get_train_test_unlabeled_for_multilabel_classification\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "HkUXA22N3Oj1",
        "outputId": "8f9360bc-e486-4cd5-c932-8e4c532e8040"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.7/dist-packages/algorithm/flexmatch.py\"\u001b[0;36m, line \u001b[0;32m118\u001b[0m\n\u001b[0;31m    return self.post_processing_and_augmentation(assigned_pseudo_labels,X,y)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We run the experiments using 10 repeated trials (in the paper we run over 30 trials)\n",
        "# We compare the results with 5 baselines: Supervised learning, Pseudo-labeling, FlexMatch, UPS, SLA and CSA\n",
        "# There are multiple datasets, we pick three of them for this report."
      ],
      "metadata": {
        "id": "fE9mhjas4cQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specify hyperparameters"
      ],
      "metadata": {
        "id": "N0ZtejZ74chA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numTrials=5\n",
        "numIters=5\n",
        "confidence_choice='ttest'\n",
        "num_XGB_models=5\n",
        "upper_threshold=0.8\n",
        "lower_threshold=0.2 # for UPS\n",
        "dataset_list=['analcatdata_authorship','dna_no','madelon_no','digits']\n",
        "algorithm_list=['supervised_learning','Pseudo_Labeling','FlexMatch','UPS','SLA','CSA']"
      ],
      "metadata": {
        "id": "iVk2Ugdx-0Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def supervised_learning(x_train, y_train, x_test, y_test):\n",
        "  param = {}\n",
        "  param['booster'] = 'gbtree'\n",
        "  param['objective'] = 'binary:logistic'\n",
        "  param['verbosity'] = 0\n",
        "  param['silent'] = 1\n",
        "  param['seed'] = 0\n",
        "\n",
        "  # create XGBoost instance with default hyper-parameters\n",
        "  xgb=XGBClassifier(**param,use_label_encoder=False)\n",
        "\n",
        "  xgb.fit(x_train, y_train)\n",
        "\n",
        "  # evaluate the performance on the test set\n",
        "  y_test_pred = xgb.predict(x_test)      \n",
        "  supervised_learning_accuracy= np.round( accuracy_score(y_test_pred, y_test)*100, 2)# round to 2 digits xx.yy %\n",
        "  return supervised_learning_accuracy"
      ],
      "metadata": {
        "id": "XCIZlW5rCDXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name='analcatdata_authorship'\n",
        "path_to_file='confident_sinkhorn_allocation/all_data.pickle'\n",
        "\n",
        "# Supervised learning==========================================================\n",
        "Accuracy_Supervised_Learning=[]\n",
        "for tt in tqdm(range(numTrials)):\n",
        "  np.random.seed(tt)\n",
        "  \n",
        "  # load the data for multiclassification\n",
        "  x_train,y_train, x_test, y_test, x_unlabeled=get_train_test_unlabeled(dataset_name,path_to_file,random_state=tt)\n",
        "\n",
        "  # train the model and get the accuracy\n",
        "  accuracy=supervised_learning(x_train, y_train, x_test, y_test)\n",
        "  Accuracy_Supervised_Learning.append(accuracy)\n",
        "\n",
        "\n",
        "# Pseudo-labeling==============================================================\n",
        "Accuracy_Pseudo_Labeling=[]\n",
        "for tt in tqdm(range(numTrials)):\n",
        "  np.random.seed(tt)\n",
        "  \n",
        "  # load the data for multiclassification\n",
        "  x_train,y_train, x_test, y_test, x_unlabeled=get_train_test_unlabeled(dataset_name,path_to_file,random_state=tt)\n",
        "\n",
        "  # train the model and get the accuracy\n",
        "  pseudo_labeller = Pseudo_Labeling(x_unlabeled,x_test,y_test, \n",
        "                num_iters=numIters, upper_threshold=upper_threshold,verbose = 0)\n",
        "  pseudo_labeller.fit(x_train, y_train)\n",
        "  Accuracy_Pseudo_Labeling.append( append_acc_early_termination(pseudo_labeller.test_acc,numIters) )\n",
        "  # append_acc_early_termination: if early termination happens, this function will copy the result to be in the same dimension\n",
        "\n",
        "\n",
        "# FlexMatch====================================================================\n",
        "Accuracy_FlexMatch=[]\n",
        "for tt in tqdm(range(numTrials)):\n",
        "  np.random.seed(tt)\n",
        "  \n",
        "  # load the data for multiclassification\n",
        "  x_train,y_train, x_test, y_test, x_unlabeled=get_train_test_unlabeled(dataset_name,path_to_file,random_state=tt)\n",
        "\n",
        "  # train the model and get the accuracy\n",
        "  pseudo_labeller = FlexMatch(x_unlabeled,x_test,y_test, \n",
        "                num_iters=numIters,upper_threshold=upper_threshold,verbose = 0)\n",
        "  pseudo_labeller.fit(x_train, y_train)\n",
        "  Accuracy_FlexMatch.append( append_acc_early_termination(pseudo_labeller.test_acc,numIters) )\n",
        "\n",
        "\n",
        "# UPS====================================================================\n",
        "Accuracy_UPS=[]\n",
        "for tt in tqdm(range(numTrials)):\n",
        "  np.random.seed(tt)\n",
        "  \n",
        "  # load the data for multiclassification\n",
        "  x_train,y_train, x_test, y_test, x_unlabeled=get_train_test_unlabeled(dataset_name,path_to_file,random_state=tt)\n",
        "\n",
        "  # train the model and get the accuracy\n",
        "  pseudo_labeller = UPS(x_unlabeled,x_test,y_test, \n",
        "                num_iters=numIters,\n",
        "                upper_threshold=upper_threshold,\n",
        "                lower_threshold=lower_threshold,\n",
        "                num_XGB_models=num_XGB_models,verbose = 0)\n",
        "  pseudo_labeller.fit(x_train, y_train)\n",
        "  Accuracy_UPS.append( append_acc_early_termination(pseudo_labeller.test_acc,numIters) )\n",
        "\n",
        "\n",
        "\n",
        "# SLA====================================================================\n",
        "Accuracy_SLA=[]\n",
        "for tt in tqdm(range(numTrials)):\n",
        "  np.random.seed(tt)\n",
        "  \n",
        "  # load the data for multiclassification\n",
        "  x_train,y_train, x_test, y_test, x_unlabeled=get_train_test_unlabeled(dataset_name,path_to_file,random_state=tt)\n",
        "\n",
        "  # train the model and get the accuracy\n",
        "  pseudo_labeller = CSA(x_unlabeled,x_test,y_test, \n",
        "                num_iters=numIters,\n",
        "                confidence_choice=None,# when setting confidence_choice=None => this is equivalent to SLA \n",
        "                num_XGB_models=num_XGB_models,verbose = 0)\n",
        "  pseudo_labeller.fit(x_train, y_train)\n",
        "  Accuracy_SLA.append( append_acc_early_termination(pseudo_labeller.test_acc,numIters) )\n",
        "\n",
        "\n",
        "# CSA====================================================================\n",
        "Accuracy_CSA=[]\n",
        "for tt in tqdm(range(numTrials)):\n",
        "  np.random.seed(tt)\n",
        "  \n",
        "  # load the data for multiclassification\n",
        "  x_train,y_train, x_test, y_test, x_unlabeled=get_train_test_unlabeled(dataset_name,path_to_file,random_state=tt)\n",
        "\n",
        "  # train the model and get the accuracy\n",
        "  pseudo_labeller = CSA(x_unlabeled,x_test,y_test, \n",
        "                num_iters=numIters,\n",
        "                confidence_choice='ttest',\n",
        "                num_XGB_models=num_XGB_models,verbose = 0)\n",
        "  pseudo_labeller.fit(x_train, y_train)\n",
        "  Accuracy_CSA.append( append_acc_early_termination(pseudo_labeller.test_acc,numIters) )\n",
        "     \n",
        "\n"
      ],
      "metadata": {
        "id": "PnWZENzL4cop"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}